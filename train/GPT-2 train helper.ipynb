{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from glob import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "\n",
    "qa_jokes_filepath = os.path.join('..', 'data', 'qa_jokes.csv')\n",
    "short_jokes_filepath = os.path.join('..', 'data', 'short_jokes.csv')\n",
    "transcripts_path = os.path.join('..', 'data', 'transcripts')\n",
    "\n",
    "qa_jokes_prep_outpath = os.path.join('..', 'data', 'prep', 'qa_jokes_gpt2.txt')\n",
    "short_jokes_prep_outpath = os.path.join('..', 'data', 'prep', 'short_jokes_gpt2.txt')\n",
    "transcripts_prep_outpath = os.path.join('..', 'data', 'prep', 'transcripts_gpt2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_encoding(s):\n",
    "    \"\"\"Skip characters that can't be encoded by standard encoder.\"\"\"\n",
    "    return s.encode('latin1', 'ignore').decode('utf8', 'ignore')\n",
    "\n",
    "def write_to_file(file_path, text):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w') as out_file:\n",
    "        out_file.write(text)\n",
    "\n",
    "START_DOC_TOKEN = ''\n",
    "END_DOC_TOKEN = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA Jokes\n",
    "Taken from [here](https://www.kaggle.com/jiriroz/qa-jokes).\n",
    "\n",
    "Cleaned from noisy/non-represantable data. (Notes, already inserted \"Q\"/\"A\" tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38234 entries, 0 to 38233\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   ID        38234 non-null  int64 \n",
      " 1   Question  38234 non-null  object\n",
      " 2   Answer    38234 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 896.2+ KB\n"
     ]
    }
   ],
   "source": [
    "qa_jokes = pd.read_csv(qa_jokes_filepath)\n",
    "qa_jokes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_corpus = ''\n",
    "for _, question, answer in qa_jokes.values:\n",
    "    qa_corpus += fix_encoding(f'{START_DOC_TOKEN}[QUESTION] {question}\\n[ANSWER] {answer}\\n{END_DOC_TOKEN}\\n')\n",
    "\n",
    "write_to_file(qa_jokes_prep_outpath, qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcripts\n",
    "Scrapped dataset of stand up's transcripts from [scrapsfromtheloft.com](scrapsfromtheloft.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_corpus = ''\n",
    "# Load transcripts.\n",
    "for file_path in glob(os.path.join(transcripts_path, '*')):\n",
    "    with open(file_path, 'r', encoding='utf8') as in_file:\n",
    "        transcript_corpus += START_DOC_TOKEN + ''.join(in_file.read()) + END_DOC_TOKEN + '\\n'\n",
    "\n",
    "transcript_corpus = fix_encoding(fix_encoding(transcript_corpus))\n",
    "\n",
    "# Save all them as dataset.\n",
    "write_to_file(transcripts_prep_outpath, transcript_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Jokes\n",
    "Dataset taken from [here](https://www.kaggle.com/abhinavmoudgil95/short-jokes).\n",
    "\n",
    "Also cleaned up. (Twitter tags, f@ck/@sshole words, samples with link to smth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 230975 entries, 0 to 230974\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   ID      230975 non-null  int64 \n",
      " 1   Joke    230975 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "short_jokes = pd.read_csv(short_jokes_filepath)\n",
    "short_jokes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find QA jokes in short jokes\n",
    "from nltk.tokenize import sent_tokenize\n",
    "qa_jokes_in_short_jokes = []\n",
    "for i, (_, joke) in enumerate(short_jokes.values):\n",
    "    sentences = sent_tokenize(joke.strip())\n",
    "    if len(sentences) < 4 and len(sentences) > 1 and sentences[0][-1] == '?':\n",
    "        qa_jokes_in_short_jokes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What do you call a black man walking down the street? A pedestrian.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show random one\n",
    "ind = np.random.randint(len(qa_jokes_in_short_jokes))\n",
    "short_jokes.iloc[qa_jokes_in_short_jokes[ind]].Joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add them to qa jokes\n",
    "for i, joke in short_jokes.iloc[qa_jokes_in_short_jokes].values:\n",
    "    sentences = sent_tokenize(joke.strip())\n",
    "    question, answer = sentences[0], ' '.join(sentences[1:])\n",
    "    qa_corpus += fix_encoding(f'{START_DOC_TOKEN}[QUESTION] {question}\\n[ANSWER] {answer}\\n{END_DOC_TOKEN}\\n')\n",
    "\n",
    "write_to_file(qa_jokes_prep_outpath, qa_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete found qa in short\n",
    "short_jokes = short_jokes.drop(qa_jokes_in_short_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_jokes_corpus = ''\n",
    "for i, joke in short_jokes.values:\n",
    "    short_jokes_corpus += fix_encoding(f'{START_DOC_TOKEN}{joke.strip()}\\n{END_DOC_TOKEN}\\n')\n",
    "\n",
    "\n",
    "write_to_file(short_jokes_prep_outpath, short_jokes_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cmd_command(python_path, script, kwargs, flags):\n",
    "    args = ' '.join(f'--{k}={v}' for k, v in kwargs.items())\n",
    "    args += ' ' + ' '.join(f'--{f}' for f in flags)\n",
    "    return f'{python_path} {script} {args}'\n",
    "\n",
    "python_path = r'C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\python.exe'\n",
    "script = r'run_language_modeling.py'\n",
    "train_kwargs = {\n",
    "    'model_type': 'gpt2', # gpt2, ctrl, openai-gpt, xlnet, transfo-xl, xlm\n",
    "    'model_name_or_path':'gpt2',\n",
    "    'output_dir':'output',\n",
    "    'block_size': 512,\n",
    "    'learning_rate': 1e-6,\n",
    "    'num_train_epochs': 3,\n",
    "    'per_gpu_train_batch_size': 2,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'save_steps': 1000,\n",
    "#     'max_steps': 20000,\n",
    "}\n",
    "\n",
    "# set CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "train_outputs = [\n",
    "    'gpt2',\n",
    "    'output',   # Transcripts 1e-5, 3\n",
    "    'output_1', # Transcripts 1e-6, 1\n",
    "    'output_2', # short_jokes 1e-5, 2\n",
    "    'output_3', # short_jokes 1e-6, 2\n",
    "    'output_4', # qa_jokes    1e-6, 3\n",
    "    'output_5', # qa_jokes    1e-5, 2\n",
    "    'output_6', # qa_jokes    1e-6, 4\n",
    "    'output_7', # qa_jokes    1e-6, 3 grad_acc 4\n",
    "    \n",
    "]\n",
    "\n",
    "train_flags = [\n",
    "    'do_train',\n",
    "#     'overwrite_output_dir',\n",
    "#     'fp16',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_kwargs['train_data_file'] = transcripts_prep_outpath\n",
    "# train_kwargs['train_data_file'] = short_jokes_prep_outpath\n",
    "train_kwargs['train_data_file'] = qa_jokes_prep_outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs['model_name_or_path'] = train_outputs[7]\n",
    "train_kwargs['output_dir'] = train_outputs[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\python.exe run_language_modeling.py --model_type=gpt2 --model_name_or_path=output_6 --output_dir=output_7 --block_size=512 --learning_rate=1e-06 --num_train_epochs=3 --per_gpu_train_batch_size=2 --gradient_accumulation_steps=4 --save_steps=1000 --train_data_file=..\\data\\prep\\qa_jokes_gpt2.txt --do_train\n"
     ]
    }
   ],
   "source": [
    "cmd_command = create_cmd_command(python_path, script, train_kwargs, train_flags)\n",
    "print(cmd_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\python.exe run_generation.py --model_type=gpt2 --model_name_or_path=output_7 --prompt=\"[QUESTION]\" --length=100 --stop_token=\"<|endoftext|>\" --temperature=0.9 --k=50 --p=0.95 --num_return_sequences=20 \n"
     ]
    }
   ],
   "source": [
    "gen_script = r'run_generation.py'\n",
    "generate_kwargs = {\n",
    "    'model_type': train_kwargs['model_type'],\n",
    "    'model_name_or_path': train_kwargs['output_dir'],\n",
    "    'prompt': rf'\"{START_DOC_TOKEN}[QUESTION]\"',\n",
    "#     'prompt': '\"The reddit enters a bar\"',\n",
    "    'length': 100,\n",
    "    'stop_token': f'\"{END_DOC_TOKEN}\"',\n",
    "    'temperature': 0.9, # temperature of 1.0 has no effect, lower tend toward greedy sampling\n",
    "#     'repetition_penalty': 1.1, # primarily useful for CTRL model; in that case, use 1.2\n",
    "    'k': 50,\n",
    "    'p': 0.95,\n",
    "#     'padding_text': '', # Padding text for Transfo-XL and XLNet.\n",
    "    'num_return_sequences':20,\n",
    "}\n",
    "gen_flags = []\n",
    "\n",
    "cmd_command = create_cmd_command(python_path, gen_script, generate_kwargs, gen_flags)\n",
    "print(cmd_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
