{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from glob import glob\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "\n",
    "# Eng\n",
    "\n",
    "qa_jokes_filepath = os.path.join('..', 'data', 'qa_jokes.csv')\n",
    "short_jokes_filepath = os.path.join('..', 'data', 'short_jokes.csv')\n",
    "transcripts_path = os.path.join('..', 'data', 'transcripts')\n",
    "\n",
    "qa_jokes_prep_outpath = os.path.join('..', 'data', 'prep', 'qa_jokes_gpt2.txt')\n",
    "short_jokes_prep_outpath = os.path.join('..', 'data', 'prep', 'short_jokes_gpt2.txt')\n",
    "transcripts_prep_outpath = os.path.join('..', 'data', 'prep', 'transcripts_gpt2.txt')\n",
    "\n",
    "# Rus\n",
    "\n",
    "rus_qa_jokes_filepath = os.path.join('..', 'data', 'rus_qa_jokes.csv')\n",
    "rus_jokes_filepath = os.path.join('..', 'data', 'rus_jokes.csv')\n",
    "rus_stories_filepath = os.path.join('..', 'data', 'anekdot_stories.csv')\n",
    "\n",
    "\n",
    "rus_qa_jokes_prep_outpath = os.path.join('..', 'data', 'prep', 'rus_qa_jokes_gpt2.txt')\n",
    "rus_jokes_prep_outpath = os.path.join('..', 'data', 'prep', 'rus_jokes_gpt2.txt')\n",
    "rus_stories_prep_outpath = os.path.join('..', 'data', 'prep', 'rus_stories_gpt2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_encoding(s):\n",
    "    \"\"\"Skip characters that can't be encoded by standard encoder.\"\"\"\n",
    "    return s.encode('utf-8', 'ignore').decode('utf8', 'ignore')\n",
    "\n",
    "# TODO: Add &amp;nbsp;  &gt;  &lt;\n",
    "regexps = [ # Regexp for the special chars\n",
    "    (re.compile('♦'), '*'),\n",
    "    (re.compile('\\n *\\n'), '\\n'), # Replace multiple newlines with one\n",
    "    (re.compile(r' {2,}'), ' '), # Replace multiple spaces with one\n",
    "]\n",
    "\n",
    "def fix_text(s):\n",
    "    for regexp in regexps:\n",
    "        s = regexp[0].sub(regexp[1], s)\n",
    "    return fix_encoding(s.strip())\n",
    "\n",
    "def write_to_file(file_path, text, encoding=None):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w', encoding=encoding) as out_file:\n",
    "        out_file.write(text)\n",
    "\n",
    "START_DOC_TOKEN = ''\n",
    "END_DOC_TOKEN = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA Jokes\n",
    "Taken from [here](https://www.kaggle.com/jiriroz/qa-jokes).\n",
    "\n",
    "Cleaned from noisy/non-represantable data. (Notes, already inserted \"Q\"/\"A\" tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38232 entries, 0 to 38231\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   ID        38232 non-null  int64 \n",
      " 1   Question  38232 non-null  object\n",
      " 2   Answer    38232 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 896.2+ KB\n"
     ]
    }
   ],
   "source": [
    "qa_jokes = pd.read_csv(qa_jokes_filepath)\n",
    "qa_jokes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_corpus = []\n",
    "for _, question, answer in qa_jokes.values:\n",
    "    qa_corpus.append(f'{START_DOC_TOKEN}[QUESTION] {question}\\n[ANSWER] {answer}\\n{END_DOC_TOKEN}')\n",
    "\n",
    "qa_corpus = '\\n'.join(map(lambda s: fix_text(s), qa_corpus))\n",
    "\n",
    "write_to_file(qa_jokes_prep_outpath, qa_corpus, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcripts\n",
    "Scrapped dataset of stand up's transcripts from [scrapsfromtheloft.com](scrapsfromtheloft.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_corpus = []\n",
    "# Load transcripts.\n",
    "for file_path in glob(os.path.join(transcripts_path, '*')):\n",
    "    with open(file_path, 'r', encoding='utf8') as in_file:\n",
    "        if len(re.findall(r'html|http|jpe?g|png|mp4', text)) > 0:\n",
    "            print('Has http|html in it:', f_path)\n",
    "        transcript_corpus.append(START_DOC_TOKEN + ''.join(in_file.read()) + END_DOC_TOKEN)\n",
    "\n",
    "transcript_corpus = '\\n'.join(map(lambda s: fix_text(s), transcript_corpus))\n",
    "\n",
    "# Save all them as dataset.\n",
    "write_to_file(transcripts_prep_outpath, transcript_corpus, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Jokes\n",
    "Dataset taken from [here](https://www.kaggle.com/abhinavmoudgil95/short-jokes).\n",
    "\n",
    "Also cleaned up. (Twitter tags, f@ck/@sshole words, samples with link to smth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 230974 entries, 0 to 230973\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   ID      230974 non-null  int64 \n",
      " 1   Joke    230974 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "short_jokes = pd.read_csv(short_jokes_filepath)\n",
    "short_jokes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find QA jokes in short jokes\n",
    "from nltk.tokenize import sent_tokenize\n",
    "qa_jokes_in_short_jokes = []\n",
    "for i, (_, joke) in enumerate(short_jokes.values):\n",
    "    sentences = sent_tokenize(joke.strip())\n",
    "    if len(sentences) < 4 and len(sentences) > 1 and sentences[0][-1] == '?':\n",
    "        qa_jokes_in_short_jokes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What do you call a Muslim organization that rejects Muhammed? A non-prophet'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show random one\n",
    "ind = np.random.randint(len(qa_jokes_in_short_jokes))\n",
    "short_jokes.iloc[qa_jokes_in_short_jokes[ind]].Joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes = {'Question': [], 'Answer': []}\n",
    "for i, joke in short_jokes.iloc[qa_jokes_in_short_jokes[ind]].values:\n",
    "    sentences = sent_tokenize(joke.strip())\n",
    "    question, answer = sentences[0], ' '.join(sentences[1:])\n",
    "    jokes['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add them to qa jokes\n",
    "for i, joke in short_jokes.iloc[qa_jokes_in_short_jokes].values:\n",
    "    sentences = sent_tokenize(joke.strip())\n",
    "    question, answer = sentences[0], ' '.join(sentences[1:])\n",
    "    qa_corpus += fix_encoding(f'{START_DOC_TOKEN}[QUESTION] {question}\\n[ANSWER] {answer}\\n{END_DOC_TOKEN}\\n')\n",
    "\n",
    "write_to_file(qa_jokes_prep_outpath, qa_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete found qa in short\n",
    "short_jokes = short_jokes.drop(qa_jokes_in_short_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_jokes_corpus = ''\n",
    "for i, joke in short_jokes.values:\n",
    "    short_jokes_corpus += fix_encoding(f'{START_DOC_TOKEN}{joke.strip()}\\n{END_DOC_TOKEN}\\n')\n",
    "\n",
    "\n",
    "write_to_file(short_jokes_prep_outpath, short_jokes_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Russian Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 110529 entries, 0 to 110528\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Text    110529 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "rus_stories = pd.read_csv(rus_stories_filepath, index_col=0)\n",
    "rus_stories.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_stories_corpus = '\\n'.join(map(lambda s: START_DOC_TOKEN + fix_text(s[0]) + END_DOC_TOKEN, rus_stories.values))\n",
    "\n",
    "# Save all them as dataset.\n",
    "write_to_file(rus_stories_prep_outpath, rus_stories_corpus, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Russian Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 439057 entries, 0 to 439056\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Text    439057 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "rus_jokes = pd.read_csv(rus_jokes_filepath, index_col=0)\n",
    "rus_jokes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Один хороший анекдот — это дополнительные 15 м...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Старик и старуха в суде. Судья: — Почему разво...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Х♦♦ — железо, пока горячо.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>— Сколько нужно вагонов, чтобы вывезти всех де...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Женщина: в 20 лет — лепестки розы в 30 лет — с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439052</th>\n",
       "      <td>Вот было бы  так: поел, завалился на диван, сп...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439053</th>\n",
       "      <td>Мишустин доездился без пропуска - вот и заболе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439054</th>\n",
       "      <td>Напоследок Мишустин из больницы пообещал сдела...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439055</th>\n",
       "      <td>План следующего выступления: 1.Ситуация с коро...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439056</th>\n",
       "      <td>- Видел обращение Путина? - В кого?!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439057 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text\n",
       "0       Один хороший анекдот — это дополнительные 15 м...\n",
       "1       Старик и старуха в суде. Судья: — Почему разво...\n",
       "2                              Х♦♦ — железо, пока горячо.\n",
       "3       — Сколько нужно вагонов, чтобы вывезти всех де...\n",
       "4       Женщина: в 20 лет — лепестки розы в 30 лет — с...\n",
       "...                                                   ...\n",
       "439052  Вот было бы  так: поел, завалился на диван, сп...\n",
       "439053  Мишустин доездился без пропуска - вот и заболе...\n",
       "439054  Напоследок Мишустин из больницы пообещал сдела...\n",
       "439055  План следующего выступления: 1.Ситуация с коро...\n",
       "439056               - Видел обращение Путина? - В кого?!\n",
       "\n",
       "[439057 rows x 1 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_jokes_corpus = '\\n'.join(map(lambda s: START_DOC_TOKEN + fix_text(s[0]) + END_DOC_TOKEN, rus_jokes.values))\n",
    "\n",
    "\n",
    "# # Save all them as dataset.\n",
    "write_to_file(rus_jokes_prep_outpath, rus_jokes_corpus, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Russian QA Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67563 entries, 0 to 67562\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  67563 non-null  int64 \n",
      " 1   Question    67563 non-null  object\n",
      " 2   Answer      67563 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "rus_qa_jokes = pd.read_csv(rus_qa_jokes_filepath)\n",
    "rus_qa_jokes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_qa_corpus = []\n",
    "for _, question, answer in rus_qa_jokes.values:\n",
    "    rus_qa_corpus.append(fix_text(f'{START_DOC_TOKEN}[ ВОПРОС] {question}\\n[ ОТВЕТ] {answer}\\n{END_DOC_TOKEN}'))\n",
    "\n",
    "rus_qa_corpus = '\\n'.join(rus_qa_corpus)\n",
    "\n",
    "write_to_file(rus_qa_jokes_prep_outpath, rus_qa_corpus, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cmd_command(python_path, script, kwargs, flags):\n",
    "    args = ' '.join(f'--{k}={v}' for k, v in kwargs.items())\n",
    "    args += ' ' + ' '.join(f'--{f}' for f in flags)\n",
    "    return f'{python_path} {script} {args}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_path = r'C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\python.exe'\n",
    "script = r'run_lm_finetuning.py'\n",
    "train_kwargs = {\n",
    "    'model_type': 'gpt2', # gpt2, ctrl, openai-gpt, xlnet, transfo-xl, xlm\n",
    "    'model_name_or_path':'gpt2',\n",
    "    'output_dir':'output',\n",
    "    'block_size': 512,\n",
    "    'learning_rate': 1e-6,\n",
    "    'num_train_epochs': 3,\n",
    "    'per_gpu_train_batch_size': 2,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'save_steps': 1000,\n",
    "#     'max_steps': 20000,\n",
    "}\n",
    "\n",
    "# set CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "train_outputs = [\n",
    "    'gpt2',\n",
    "    'output',   # Transcripts 1e-6, 5 grad_acc 4\n",
    "    'output_1', # Transcripts 1e-5, 2 grad_acc 8\n",
    "    'output_2', # short_jokes 1e-5, 2 grad_acc 8\n",
    "    'output_3', # short_jokes 1e-6, 5 grad_acc 8\n",
    "    'output_4', # short_jokes 1e-7, 2 grad_acc 2\n",
    "    'output_5', # qa_jokes    1e-5, 3 grad_acc 8 - most funny yet\n",
    "    'output_6', # qa_jokes    1e-5, 3 grad_acc 4\n",
    "    'output_7', # qa_jokes    1e-6, 2 grad_acc 2\n",
    "    'output_8', # qa_jokes    1e-6, 10 grad_acc 8\n",
    "    \n",
    "]\n",
    "\n",
    "train_flags = [\n",
    "    'do_train',\n",
    "    'overwrite_output_dir',\n",
    "#     'fp16',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_kwargs['train_data_file'] = transcripts_prep_outpath\n",
    "# train_kwargs['train_data_file'] = short_jokes_prep_outpath\n",
    "train_kwargs['train_data_file'] = qa_jokes_prep_outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: gpt2 \n",
      "To: models\\output\n"
     ]
    }
   ],
   "source": [
    "train_kwargs['model_name_or_path'] = train_outputs[0]\n",
    "# train_kwargs['model_name_or_path'] = os.path.join('models', train_outputs[0])\n",
    "train_kwargs['output_dir'] = os.path.join('models', train_outputs[1])\n",
    "print('From:', train_kwargs['model_name_or_path'], '\\nTo:', train_kwargs['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\python.exe ru_transformers-master\\run_lm_finetuning.py --model_type=gpt2 --model_name_or_path=gpt2 --output_dir=models\\output --block_size=512 --learning_rate=1e-06 --num_train_epochs=3 --per_gpu_train_batch_size=2 --gradient_accumulation_steps=8 --save_steps=1000 --train_data_file=..\\data\\prep\\qa_jokes_gpt2.txt --do_train --overwrite_output_dir\n"
     ]
    }
   ],
   "source": [
    "cmd_command = create_cmd_command(python_path, script, train_kwargs, train_flags)\n",
    "print(cmd_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\python.exe run_generation.py --model_type=gpt2 --model_name_or_path=models\\rus_2 --prompt=\"[QUESTION]\" --length=100 --stop_token=\"<|endoftext|>\" --temperature=0.9 --repetition_penalty=1.05 --k=50 --p=0.95 --num_return_sequences=40 \n"
     ]
    }
   ],
   "source": [
    "gen_script = r'run_generation.py'\n",
    "generate_kwargs = {\n",
    "    'model_type': train_kwargs['model_type'],\n",
    "    'model_name_or_path': train_kwargs['output_dir'],\n",
    "    'prompt': rf'\"{START_DOC_TOKEN}[QUESTION]\"',\n",
    "#     'prompt': '\"The reddit enters a bar\"',\n",
    "    'length': 100,\n",
    "    'stop_token': f'\"{END_DOC_TOKEN}\"',\n",
    "    'temperature': 0.9, # temperature of 1.0 has no effect, lower tend toward greedy sampling\n",
    "    'repetition_penalty': 1.05, # primarily useful for CTRL model; in that case, use 1.2\n",
    "    'k': 50,\n",
    "    'p': 0.95,\n",
    "#     'padding_text': '', # Padding text for Transfo-XL and XLNet.\n",
    "    'num_return_sequences':40,\n",
    "}\n",
    "gen_flags = []\n",
    "\n",
    "cmd_command = create_cmd_command(python_path, gen_script, generate_kwargs, gen_flags)\n",
    "print(cmd_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ru_transformers\n",
    "https://github.com/mgrankin/ru_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_path = r'C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\python.exe'\n",
    "script = r'run_lm_finetuning.py'\n",
    "train_kwargs = {\n",
    "    'model_type': 'gpt2-yttm', # gpt2, ctrl, openai-gpt, xlnet, transfo-xl, xlm\n",
    "    'model_name_or_path':'gpt2',\n",
    "    'output_dir':'output',\n",
    "    'block_size': 512,\n",
    "    'learning_rate': 5e-7,\n",
    "    'num_train_epochs': 5,\n",
    "    'per_gpu_train_batch_size': 2,\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'save_steps': 1000,\n",
    "    'save_total_limit': 3,\n",
    "    'logging_steps': 10,\n",
    "    'warmup_samples': 500,\n",
    "    'unfreeze_level': -1,\n",
    "#     'max_steps': 20000,\n",
    "}\n",
    "\n",
    "train_outputs = [\n",
    "    r'ru_gpt2/s_checkpoint-1900000', # 'ru_gpt2\\m_checkpoint-3364613'\n",
    "    'rus_test_1', # rus_jokes 1 1e-4 16 unfreeze 0\n",
    "    'rus_test_2', # rus_jokes 1 5e-5 16 unfreeze 1\n",
    "    'rus_test_3', # rus_jokes 1 5e-5 16 unfreeze 2\n",
    "    'rus_test_4', # rus_jokes 1 1e-4 16 unfreeze 7\n",
    "    'rus_test_5', # rus_jokes 2 5e-6 16 unfreeze -1\n",
    "    'rus_test_6', # rus_qa_jokes \n",
    "    'rus_test_7', # rus_qa_jokes\n",
    "]\n",
    "\n",
    "\n",
    "train_flags = [\n",
    "    'do_train',\n",
    "    'overwrite_output_dir',\n",
    "    'lr_decay',\n",
    "#     'fp16',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\prep\\rus_qa_jokes_gpt2.txt\n"
     ]
    }
   ],
   "source": [
    "# train_kwargs['train_data_file'] = rus_stories_prep_outpath\n",
    "# train_kwargs['train_data_file'] = rus_jokes_prep_outpath\n",
    "train_kwargs['train_data_file'] = rus_qa_jokes_prep_outpath\n",
    "print(train_kwargs['train_data_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: models\\rus_test_6 \n",
      "To: models\\rus_test_7\n"
     ]
    }
   ],
   "source": [
    "# train_kwargs['model_name_or_path'] = train_outputs[0]\n",
    "train_kwargs['model_name_or_path'] = os.path.join('models', train_outputs[6])\n",
    "\n",
    "# train_kwargs['tokenizer_name'] = train_kwargs['tokenizer_name'].format(train_kwargs['model_name_or_path'])\n",
    "train_kwargs['output_dir'] = os.path.join('models', train_outputs[7])\n",
    "print('From:', train_kwargs['model_name_or_path'], '\\nTo:', train_kwargs['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\python.exe ru_transformers-master\\run_lm_finetuning.py --model_type=gpt2-yttm --model_name_or_path=models\\rus_test_6 --output_dir=models\\rus_test_7 --block_size=512 --learning_rate=5e-07 --num_train_epochs=5 --per_gpu_train_batch_size=2 --gradient_accumulation_steps=16 --save_steps=1000 --save_total_limit=3 --logging_steps=10 --warmup_samples=500 --unfreeze_level=-1 --train_data_file=..\\data\\prep\\rus_qa_jokes_gpt2.txt --do_train --overwrite_output_dir --lr_decay\n"
     ]
    }
   ],
   "source": [
    "cmd_command = create_cmd_command(python_path, script, train_kwargs, train_flags)\n",
    "print(cmd_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    --do_eval \\\n",
    "    --evaluate_during_training \\\n",
    "    --eval_steps 1000 \\\n",
    "    --eval_data_file=./data/classic/valid \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\pytorch\\python.exe ru_transformers-master\\run_generation.py --model_type=gpt2-yttm --model_name_or_path=models\\output --length=200 --temperature=0.9 --stop_token=\"<|endoftext|>\" --top_k=50 --top_p=0.95 --num_return_sequences=20 \n"
     ]
    }
   ],
   "source": [
    "gen_script = r'run_generation.py'\n",
    "generate_kwargs = {\n",
    "    'model_type': 'gpt2-yttm',\n",
    "    'model_name_or_path': train_kwargs['output_dir'],\n",
    "    'length': 200,\n",
    "    'temperature': 0.9, # temperature of 1.0 has no effect, lower tend toward greedy sampling\n",
    "    'stop_token': f'\"{END_DOC_TOKEN}\"',\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.95,\n",
    "    'num_return_sequences':20,\n",
    "}\n",
    "gen_flags = []\n",
    "\n",
    "cmd_command = create_cmd_command(python_path, gen_script, generate_kwargs, gen_flags)\n",
    "print(cmd_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
