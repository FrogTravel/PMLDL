import os
import re
import random
import threading
import logging
from abc import ABC, abstractmethod

import storage
from inference import ModelWrapper
from data import Dataset, Joke


def synchronized(func):
    """Decorator for the syncronized usage of the function."""
    func.__lock__ = threading.Lock()

    def synced_func(*args, **kws):
        with func.__lock__:
            return func(*args, **kws)

    return synced_func


class AbstractJokeGenerator(ABC):
    """Abstract class for joke generation using `ModelWrapper`."""

    POS_GRADE = 1
    NEG_GRADE = -1

    def_config = {
        'buffer_size': 16,
        'max_len': 50,
        'device': 'cpu',
        'promt_token': '[QUESTION]',
        'answer_token': '[ANSWER]',
        'custom_promt': '[QUESTION]{}\n[ANSWER]',
        'stop_token': '<|endoftext|>',
    }

    def __init__(self, config=None):
        self.store = storage
        self.config = AbstractJokeGenerator.def_config.copy()
        if config:
            self.config.update(config)

    def positive_grade(self, user_id, joke_id):
        self.store.add_or_update_vote(
            joke_id=joke_id, user_id=user_id, rating=self.POS_GRADE)

    def negative_grade(self, user_id, joke_id):
        self.store.add_or_update_vote(
            joke_id=joke_id, user_id=user_id, rating=self.NEG_GRADE)

    def _escape_markdown(self, text):
        """Escape the possible markdown tokens in the text generated by model."""
        return re.sub(r'((([_*]).+?\3[^_*]*)*)([_*])', "\g<1>\\\\\g<4>", text)

    def _prettify_result(self, model_output):
        def pp_answer(text):
            """Pretty-print the answer."""
            # Remove all text after the stop token.
            text = text[: text.find(self.config['stop_token'])
                        if self.config['stop_token'] else None]
            # Remove multiple answers.
            text = self.config['answer_token'].join(text.split(self.config['answer_token'], 2)[:2])
            # Escape markdown tokens.
            text = self._escape_markdown(text)
            # Replace model tokens with html formatted ones.
            text = re.sub(f'\{self.config["promt_token"]} *', '*Question*: ', text)
            text = re.sub(f'\{self.config["answer_token"]} *', '\n*Answer*: ', text)
            return text

        if isinstance(model_output, str):
            return pp_answer(model_output)
        else:
            return [pp_answer(ans) for ans in model_output]

    def generate_joke(self, model, promt=""):
        """Generate the joke from given promt.

        :param model: model to use to generate joke.
        :param promt: (optional) promt for a joke, if not given
        generates the whole joke
        :return: `Joke` object
        """
        if promt:
            res = self._continue_joke(model, promt)
        else:
            res = self._get_new_joke()
        res['text'] = self._prettify_result(res['text'])
        joke_id = self.store.add_joke(**res)
        return Joke(id=joke_id, text=res['text'])

    @synchronized
    def __call_model(self, model, prompt, n_return_sequences):
        """Call the model to generate the joke.

        :param model: model to use
        :param promt: prompt for the model
        :param n_return_sequences: number of sequences to generate
        :return: list of (n_return_sequences) dicts
        with 'text' and 'generated_by' fields
        """
        return [{
            'generated_by': model.name,
            'text': seq
        } for seq in model.generate(prompt, n_return_sequences)]

    @synchronized
    def _get_from_buffer(self, model, buffer):
        """Get a joke from the buffer.

        :param buffer: buffer to get the joke from
        :param model: model to use to fill the buffer
        :return: joke from the buffer
        """
        if len(buffer) == 0:
            buffer = self._fill_buffer(model)
        model.logger.info('Got joke from the buffer')
        return buffer.pop()

    @synchronized
    @abstractmethod
    def _fill_buffer(self, model):
        """Fill and return the buffer associated with given model.
        """
        pass

    @synchronized
    def _generate_for_buffer(self, model):
        """Generates jokes to fill the buffer.

        :param model: model to use
        :return: see `__call_model` function
        """
        model.logger.info('Filling the buffer')
        return self.__call_model(model, self.config['promt_token'],
                                 n_return_sequences=self.config['buffer_size'])

    @abstractmethod
    def _get_new_joke(self, model):
        """Get the new joke from the given model.
        """
        pass

    @synchronized
    def _continue_joke(self, model, promt):
        """Continue the joke given in promt."""
        model.logger.info('Continue joke')
        model_promt = self.config['custom_promt'].format(' ' + promt.strip())
        return self.__call_model(model, model_promt, n_return_sequences=1)[0]


class JokeGenerator(AbstractJokeGenerator):
    """Simple Joke generator using one model."""

    def __init__(self, model_path, config):
        super().__init__(config)
        model_name = os.path.split(model_path)[1]
        self.logger = logging.getLogger(type(self).__name__)
        self.logger.info('Loading model...')
        self.model = ModelWrapper(model_path, model_name, **config)
        self.logger.info(f'Loaded {self.model.name}')
        self._fill_buffer(self.model)
        self.logger.info('Ready to work!')

    def generate_joke(self, promt=""):
        return super().generate_joke(self.model, promt)

    @synchronized
    def _fill_buffer(self, model):
        self.jokes_buffer = super()._generate_for_buffer(model)
        return self.jokes_buffer

    def _get_new_joke(self):
        return self._get_from_buffer(self.model, self.jokes_buffer)


class TestABGenerator(AbstractJokeGenerator):
    """Joke generator for a/b testing.
    Outputs the joke from either of models/datasets.
    Chooses the source randomly.
    """

    def __init__(self, dataset_paths, model_paths, config):
        """
        Loads datasets and models. Initiates pools and orders of passing
        :param dataset_paths: paths to the dataset
        :param model_paths: paths to the model
        """
        super().__init__(config)
        self.models, self.key2pool = list(), dict()
        self.logger = logging.getLogger(type(self).__name__)
        self.logger.info('Loading models...')
        for m_path in model_paths:
            m_name = os.path.split(m_path)[1]
            self.models.append(ModelWrapper(m_path, m_name, **config))
            self.logger.info(f'Loaded {self.models[-1].name}')
            self._fill_buffer(self.models[-1])

        self.logger.info('Loading datasets...')
        self.datasets = list()
        for d_path in dataset_paths:
            self.datasets.append(Dataset(d_path, self.config['promt_token'],
                                         self.config['answer_token']))
            self.logger.info(f'Loaded {self.datasets[-1].name}')
        self.logger.info('Ready to work!')
        self.n_pools = len(self.models) + len(self.datasets)

    def generate_joke(self, promt=""):
        idx = random.randint(0, len(self.models) - 1)
        model = self.models[idx]
        return super().generate_joke(model, promt)

    @synchronized
    def _fill_buffer(self, model):
        self.key2pool[model.name] = super()._generate_for_buffer(model)
        return self.key2pool[model.name]

    def _get_new_joke(self):
        """Get the joke either from the model buffer, or dataset."""
        idx = random.randint(0, self.n_pools - 1)
        if idx < len(self.datasets):
            return random.choice(self.datasets[idx])
        idx = idx - len(self.datasets) - 1
        key = self.models[idx].name
        return self._get_from_buffer(self.models[idx],
                                     self.key2pool[key])


class RussianModelWrapper(AbstractJokeGenerator):

    def __init__(self, eng_model, rus_model_path, config):
        super().__init__(config)
        self.eng_model = eng_model
        config.update({ # Spaces are because of the tokenizer
            'promt_token': '[ ВОПРОС]',
            'answer_token': '[ ОТВЕТ]',
            'custom_promt': '[ ВОПРОС]{}\n[ ОТВЕТ]',
            'stop_token': '<| endoftext|>',
            'model_type': 'gpt2-yttm',
        })
        self.rus_model = JokeGenerator(rus_model_path, config)

    @staticmethod
    def cyrillic_ratio(text):
        return sum(map(len, re.findall('[\u0400-\u04FF]*', text))) / len(text)

    def generate_joke(self, promt=""):
        """If the ratio of cyrillic symbols if high enough,
        generate using russian model."""
        if promt.strip() == '/шутка':
            return self.rus_model.generate_joke('')
        if promt and self.cyrillic_ratio(promt) > 0.4:
            return self.rus_model.generate_joke(promt)
        return self.eng_model.generate_joke(promt)

    @synchronized
    def _fill_buffer(self, model):
        assert False, "Unreachable code. _fill_buffer function in RussianModelWrapper."

    def _get_new_joke(self):
        """Get the joke from the english model."""
        return self.eng_model._get_new_joke()


if __name__ == '__main__':
    logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                        level=logging.INFO)

    datasets = ["../data/qa_jokes.csv"]
    models = ["../train/models/output_8"]

    gen = TestABGenerator(datasets, models)

    for i in range(10):
        print(gen.generate_joke().text)
